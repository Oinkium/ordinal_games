\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc}

\usepackage{graphicx} % support the \includegraphics command and options

\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)\prefix\t$.
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{mathtools} % for the all important \coloneqq symbol
\usepackage{url} % for \url
\usepackage{IEEEtrantools} % for \IEEEeqnarray
\usepackage{pbox} % for \pbox

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%% Font things %%
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{cmll} % Linear logic symbols!

%% Lists %%
\usepackage{enumerate}

%% Graphics %%
\usepackage{tikz}
\usetikzlibrary{cd}
\usetikzlibrary{patterns}
\usetikzlibrary{calc}

%% Theorems! %%
\usepackage{amsthm}
\theoremstyle{plain} % Theorems, lemmas, propositions etc.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{construction}[theorem]{Construction}
\theoremstyle{definition} % Definitions etc.  Remarks too, because I don't like the way the 'remark' style looks.
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}[theorem]{Question}

%% Exercises and answers %%
\usepackage{answers}

\newtheoremstyle{exercisestyle}% name
  {6pt}   % ABOVESPACE
  {6pt}   % BELOWSPACE
  {\itshape}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {3pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC

\theoremstyle{exercisestyle}
\newtheorem{exercise}{Exercise}
\newtheorem{answerthm}{Exercise}

\Newassociation{answer}{answerthm}{answers}
\newcommand{\answerthmparams}{}

%% Changes to enumerate things so they look better %%\sigma$

\makeatletter
\def\enumfix{%
\if@inlabel
 \noindent \par\nobreak\vskip-\topsep\hrule\@height\z@
\fi}

\let\olditemize\itemize
\def\itemize{\enumfix\olditemize}
\let\oldenumerate\enumerate
\def\enumerate{\enumfix\oldenumerate}

%% Random crap %%
\usepackage{xifthen}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=\parskip \thm@postskip=0pt
}
\makeatother

\makeatletter
\newcommand*{\relrelbarsep}{.386ex}
\newcommand*{\relrelbar}{%
  \mathrel{%
    \mathpalette\@relrelbar\relrelbarsep
  }%
}
\newcommand*{\@relrelbar}[2]{%
  \raise#2\hbox to 0pt{$\m@th#1\relbar$\hss}%
  \lower#2\hbox{$\m@th#1\relbar$}%
}
\providecommand*{\rightrightarrowsfill@}{%
  \arrowfill@\relrelbar\relrelbar\rightrightarrows
}
\providecommand*{\leftleftarrowsfill@}{%
  \arrowfill@\leftleftarrows\relrelbar\relrelbar
}
\providecommand*{\xrightrightarrows}[2][]{%
  \ext@arrow 0359\rightrightarrowsfill@{#1}{#2}%
}
\providecommand*{\xleftleftarrows}[2][]{%
  \ext@arrow 3095\leftleftarrowsfill@{#1}{#2}%
}
\makeatother

\newcommand{\catname}[1]{{\normalfont\textbf{#1}}}
\newcommand{\Rings}{\catname{CRing}}
\newcommand{\CAT}{\catname{CAT}}
\newcommand{\Top}{\catname{Top}}
\newcommand{\Set}{\catname{Set}}
\newcommand{\Cont}{\catname{Cont}}
\newcommand{\Sch}{\catname{Sch}}
\newcommand{\Rel}{\catname{Rel}}
\newcommand{\Mod}[1][]{\ifthenelse{\isempty{#1}}{\catname{Mod}}{#1\catname{mod}}}
\DeclareMathOperator{\sh}{Sh}
\newcommand{\Sh}[1][]{\ifthenelse{\isempty{#1}}{\sh}{\sh(#1)}}
\newcommand{\map}[3]{#2\xrightarrow{#1} #3}
\newcommand*\from{\colon}
\newcommand{\cmap}[3]{#1\from{}#2\to{}#3}
\newcommand\oppcat[1]{#1^{\mathrm{op}}}
\DeclareRobustCommand{\vmap}[3] {\begin{tikzcd} #2 \arrow[d, "#1"] \\ #3 \end{tikzcd}}
\newcommand{\partref}[1]{(\ref{#1})}
\newcommand{\intgrpd}[4] {#1 \xrightrightarrows[#3]{#4} #2}
\DeclareRobustCommand{\bigintgrpd}[4] {\begin{tikzcd}[ampersand replacement=\&] #1 \arrow[r, shift left=0.5ex, "#3"] \arrow[r, shift right=0.5ex, "#4"'] \& #2 \end{tikzcd}}

\usepackage{xspace}

\newcommand{\etale}{\'{e}tale\xspace}
\newcommand{\Etale}{\'{E}tale\xspace}

\def \inv {^{-1}}

\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\op}{op}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\pre}{{pre}}
\DeclareMathOperator{\et}{{\acute{e}t}}

\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Spec}{Spec}

\DeclareMathOperator{\ol}{ol}

\def\presuper#1#2%
  {\mathop{}%
   \mathopen{\vphantom{#2}}^{#1}%
   \kern-\scriptspace%
   #2}
\def\presub#1#2%
  {\mathop{}%
   \mathopen{\vphantom{#2}}_{#1}%
   \kern-\scriptspace%
   #2}

%% Our things %%

\newcommand{\neggame}[1]{\presuper{\perp}{#1}}
\newcommand{\tensor}{\otimes}
\newcommand{\sequoid}{\oslash}
\newcommand{\varsequoid}{\vartriangleleft}
\renewcommand{\implies}{\multimap}
\newcommand{\comp}[2]{#1 \circ #2}
\newcommand{\cprd}{\sqcup}
\newcommand{\G}{\mathcal G}
\newcommand{\suchthat}{\;\colon\;}
\newcommand{\varsuchthat}{\;\mid\;}
\newcommand{\esuchthat}{\;.\;}
\newcommand{\OP}{\{O,P\}}
\renewcommand{\L}{\mathcal L}
\newcommand{\F}{\mathcal F}
\newcommand{\s}{\mathfrak s}
\renewcommand{\t}{\mathfrak t}
\newcommand{\emptyplay}{\epsilon}
\newcommand{\bracketed}[1]{\left({#1}\right)}
\newcommand{\bneggame}[1]{\bracketed{\neggame{#1}}}
\newcommand{\prefix}{\sqsubseteq}
\renewcommand{\ss}{\mathbf{s}}
\newcommand{\pfun}{\rightharpoonup}
\newcommand{\grel}[1]{\underline{#1}}
\DeclareMathOperator{\length}{length}

%%% END Article customizations

\begin{document}

\section{Introduction}

These are games with ordinal sequences of moves.

TODO: Talk about what they are and why we are interested, being careful to point out that our $\omega+1$ games correspond to games with winning conditions.

\section{Our starting category of games}

Before studying games with transfinite sequences of moves, we shall illustrate some of the choices we have made by defining a category of games with finite sequences of moves.  We have chosen these definitions because they extend particularly well to the transfinite case.  

\subsection{Games and strategies}

We shall use the notation introduced in \cite{abramskyjagadeesangames} to describe games.  All our games $A$ will have, at their heart, the following three pieces of information:
\begin{itemize}
  \item A set $M_A$ of possible moves
  \item A function $\cmap{\lambda_A}{M_A}{\OP}$ assigning to each move the player who is allowed to make that move
  \item A prefix-closed set $P_A\subset M_A^*$ of finite sequences of moves.
\end{itemize}
We shall normally insist on an \emph{alternating condition} on $P_A$:
\begin{description}
  \item[Alternating condition] If $a,b\in M_A$ are moves and $s\in M_A^*$ is a sequence of moves such that $sa, sab\in P_A$, then $\lambda_A(a)=\neg\lambda_A(b)$.
\end{description}

As in \cite{abramskyjagadeesangames}, we identify a \emph{strategy} for a game $A$ with the set of sequences of moves that can occur when player $P$ is playing according to that strategy so that a typical definition of a (partial) strategy might be a set $\sigma\subset P_A$ such that (for all $s\in M_A^*, a,b\in M_A$):
\begin{itemize}
  \item $\emptyplay\in\sigma$ (ensures that $\sigma$ is non-empty)
  \item If $sa\in\sigma$, $\lambda_A(a)=P$ and $sab\in P_A$ then $sab\in\sigma$ ($\sigma$ contains all legal replies by player $O$)
  \item If $s,sa,sb\in\sigma$ and $\lambda_A(a)=P$ then $a=b$ ($\sigma$ contains at most one legal reply by player $P$)
\end{itemize}

We can impose additional constraints on $\sigma$ that will ensure that $\sigma$ is total, strict, history free and so on.  The definition given immediately above is not the only definition of a strategy found in the literature, however.  For example, the games described in \cite{abramskyjagadeesangames} have the curious property that the set $P_A$ may contain plays that cannot actually occur when $A$ is being played; in particular, all plays must start with a move by player $O$, but the set $P_A$ may contain positions that start with a $P$-move.  These plays do not affect the strategies for $A$, but they might come into play if we perform operations on $A$ such as forming the negation $\neg A$ or the implication $A\implies B$.  

This behaviour is made implicit in Abramsky and Jagadeesan's definitions, which do not impose any conditions upon the set $P_A$ beyond the basic alternation condition given above, but which mandate that any play occurring \emph{in a strategy} must begin with an $O$-move.  For the sake of clarity, we adopt a different, but completely equivalent, approach.  For a game $A$, we define a set $L_A$, regarded as the set of \emph{legal plays} occurring in $P_A$.  In some games models, such as that found in \cite{blassgames}, $L_A$ may be defined to be the whole of $P_A$, while in \cite{abramskyjagadeesangames} it is defined to be that subset of $P_A$ consisting of plays that begin with an $O$-move.

The point of specifying $L_A$ separately is that it allows us to unify the definitionn of a \emph{strategy}, while making clearer the behaviour observed above, whereby certain plays in $P_A$ may not occur `in normal play'; this behaviour was previously only implicit in the definition of a strategy.  Our unified definition then becomes:

\begin{definition}
  If $A=(M_A,\lambda_A,P_A)$ is a game and $L_A$ is its associated set of legal plays (in a particular games model) then a (partial) \emph{strategy} for $A$ is a subset $\sigma\subset L_A$ such that for all $s\in L_A$ and all $a, b\in M_A$:
  \begin{itemize}
    \item $\emptyplay\in\sigma$
    \item If $s\in\sigma$ and $a$ is an $O$-move, and if $sa\in L_A$, then $sa\in\sigma$
    \item If $s\in\sigma$ and $a,b$ are $P$-moves, and if $sa,sb\in\sigma$, then $a=b$
  \end{itemize}
\end{definition}

\subsection{Positive and negative games, ownership of plays and connectives}

Abramsky-Jagadeesan games, as described in \cite{abramskyjagadeesangames}, may admit both plays that start with a $P$-move and plays that start with an $O$-move.  Other games models, such as those found in \cite{blassgames} and \cite{curiengames}, are more restrictive.  The games in \cite{curiengames} only contain plays starting with an $O$-move.  The plays in \cite{blassgames} may start with either a $P$-move or an $O$-move, but a play starting with a $P$-move and a play starting with an $O$-move may not occur in the same game.

\begin{definition}
  We say that a game $A=(M_A,\lambda_A,P_A)$ is \emph{positive} if every play in $P_A$ begins with a $P$-move.
  We say that $A$ is \emph{negative} if every play in $P_A$ begins with an $O$-move.
\end{definition}

So the Curien model found in \cite{curiengames} admits only negative games, the Blass model in \cite{blassgames} admits positive and negative games, while the Abramsky-Jagadeesan model found in \cite{abramskyjagadeesangames} admits not only positive and negative games, but also games that are neither negative nor positive.  We shall now examine the reasons for and drawbacks of each of these choices.

The earliest games model, found in \cite{conwaygames}, did not include a definition of which player is to move at a given position; rather, games are defined recursively as pairs of games $\{L|R\}$, where $L$ represents the positions that the left player may move into, while $R$ represents the positions that the rigth player may move into.  Blass's definition departs completely from this tradition; now, at every position $s$ only one of the two players is allowed to move; extending this logic on to the empty position $\emptyplay$, it follows that all games are either positive or negative.  This property means that we may freely define $L_A=P_A$, since there is never any question about whose turn it is to play.  By contrast, if we were to define $L_A=P_A$ for Abramsky-Jagadeesan games, then a strategy might end up containing two branches, one of plays beginning with an $O$-move and one of plays beginning with a $P$-move, which is undesirable.  The alternative definition of $L_A$ avoids this problem.

In the case of a Blass game $A$, we may define a function $\zeta_A\colon P_A\to\OP$ that says which player owns each play; the idea is that if we are in position $s$, then the next player to move is given by $\neg\zeta_A(s)$; i.e., the opposing player to the player who has just made the move.  One might want to define $\zeta_A$ by setting $\zeta_A(sa)=\lambda_A(a)$, so that ownership of a play is decided by who has made the last move in the play, but this definition does not extend in an obvious way to the empty position $\emptyplay$ (and, as we shall see in the next chapter, it does not extend to plays over limit ordinals).  In this case, $\zeta_A(\emptyplay)$ is part of the game's data, and it determines whether the game is positive or negative: if $\zeta_A(\emptyplay) = P$ then all plays must start with an $O$-move, and the game is negative -- and vice versa.

An important question then arises: how should we extend the function $\zeta_A$ to games formed from connectives?  The solution adopted by Blass is to use binary conjunctions to deduce the ownership of a play from the ownership of the restrictions of that play to the two component games.  In the case of the tensor product $A\tensor B$ of two games $A$ and $B$, we define $\cmap{\zeta_{A\tensor B}}{P_{A\tensor B}}{\OP}$ by setting
\[
  \zeta_{A\tensor B}(s) = (\zeta_A(s\vert_A) \wedge \zeta_B(s\vert_B))
  \]
where $\cmap{\wedge}{\OP\times \OP}{\OP}$ is as in Figure \ref{truthtables}.

\begin{figure}[h]
  \begin{center}
    $\begin{array}{cc|c}
      a & b & a \wedge b \\
      \hline
      O & O & O \\
      O & P & O \\
      P & O & O \\
      P & P & P
    \end{array}$
    \quad
    $\begin{array}{cc|c}
      a & b & a \vee b \\
      \hline
      O & O & O \\
      O & P & P \\
      P & O & P \\
      P & P & P
    \end{array}$
    \quad
    $\begin{array}{cc|c}
      a & b & a \Rightarrow b \\
      \hline
      O & O & P \\
      O & P & P \\
      P & O & O \\
      P & P & P
    \end{array}$
    \caption{Truth tables for binary conjunctions on $\OP$}
    \label{truthtables}
  \end{center}
\end{figure}

Similarly, we may extend $\zeta$ to the implication $A\implies B$ and the par $A\parr B$ by setting
\begin{align*}
  \zeta_{A\implies B}(s)=(s\vert_A\Rightarrow s\vert_B)\\
  \zeta_{A\parr B}(s) = (s\vert_A \vee s\vert_B)
\end{align*}

Note that if we use these definitions then the owner $\zeta_C(sa)$ of a play $sa$ might not correspond to the player $\lambda_C(a)$ who played the last move $a$.  For example, let $A,B$ be two positive games and form their tensor product $A\tensor B$.  Then we have
\[
  \zeta_{A\tensor B}(\emptyplay) = (\zeta_A(\emptyplay) \wedge \zeta_B(\emptyplay)) = O \wedge O = O
  \]
and so $A\tensor B$ is a positive game.  Player $P$ plays an opening move in one of the two games - let us say she plays the move $a$ in the game $A$.  But then we have
\[
  \zeta_{A\tensor B}(a) = (\zeta_A(a) \wedge \zeta_B(\emptyplay)) = P \wedge O = O
  \]
In other words, it is still player $P$'s turn to play!  Blass embrace this possibility and allows player $P$ to make these two moves.  In his paper, he introduces the notions of \emph{strict} and \emph{relaxed} games, where the strict games are the objects of study but the relaxed games are often used since they allow more manipulations.  In this case, the game $A\tensor B$ is defined as a relaxed game that might not satisfy the alternating condition; in the process of converting it into a strict game, these two opening moves by player $P$ are combined into a single move.

This `double move' can only occur at the start of the game, and Blass treats it as a special case in his proofs.  Perhaps unsurprisingly, this inconsistency causes major problems if we try to compose strategies.  We do not get an associative composition of strategies for $A\implies B$ with strategies for $B\implies C$ and so we do not get a categorical semantics.  An example of the failure of associativity in Blass's games model is given towards the end of \cite{abramskyjagadeesangames}.

By contrast, Abramsky-Jagadeesan games may admit moves by both players at the same position (specifically, at the beginning of the game, before any moves have been played), but this does not cause problems since we insist that our legal plays start with an $O$-move and be strictly alternating.  The authors of \cite{abramskyjagadeesangames} note that their model can be considered as an intermediate between Conway's games, where the position tells you nothing about which player is to move, and Blass games, where the position completely determines which player is to move.  In Abramsky-Jagadeesan games, one can deduce which player is to move (by looking at which player made the last move) in every position except the empty starting position.

In the Abramsky-Jagadeesan model, a positive game is an immediate win to player $P$, since player $O$ has no legal move to start the game off.  As we noted before, this does not mean that the content of a positive game is meaningless, since we can use connectives to `unlock' these illegal plays.  For example, if $Q$ is a positive game and $N$ is a negative game then $Q\parr N$ is a negative game, and the possible positions in $Q$ are now all achievable.  

Curien's game model (\cite{curiengames}) is similar to Abramsky's and Jagadeesan's, but involves only negative games.  The only slight problem is that negative Abramsky-Jagadeesan games are not closed under implication: if $N,L$ are negative games then $N\implies L$ may be neither negative nor positive.  We may fix this by modifying the definition of $N\implies L$ so that we delete from $P_{N\implies L}$ all plays that start with a $P$-move - or, equivalently, by requiring that all plays start in $L$.  This is the approach taken in \cite{martinsthesis}, where it fits well with the paper's treatment of the \emph{sequoid} operator $\sequoid$, which is a version of the tensor product that has been modified so that play is required to start in the left-hand game.

We shall adopt elements of both the Blass and the Abramsky-Jagadeesan games models; specifically, we shall use Blass's games and Abramsky-Jagadeesan's strategies.  This means that our games model will be more restrictive than either the Blass or the Abramsky-Jagadeesan models, but this lack of flexibility will be just what we need in order to extend these games over the transfinite ordinals.  We will later consider ways we can relax our model to recover Abramsky and Jagadeesan's games model.

\subsection{Our definition of games and strategies}

\begin{definition}
  A \emph{game} is a triple $(M_A,\lambda_A,\zeta_A,P_A)$ where
  \begin{itemize}
    \item $M_A$ is a set of moves,
    \item $\cmap{\lambda_A}{M_A}{\OP}$ is a function that assigns a player to each move,
    \item $P_A\subset M_A^*$ is a non-empty prefix-closed set of plays that can occur in the game and
    \item $\cmap{\zeta_A}{P_A}{\OP}$ is a function that assigns a player to each position
  \end{itemize}
  such that
  \begin{itemize}
    \item If $a\in M_A$ and $sa\in P_A$ then $\zeta_A(sa)=\lambda_A(a)$.
    \item If $a\in M_A$ and $sa\in P_A$ then $\zeta_A(s)=\neg\zeta_A(sa)$.
  \end{itemize}
\end{definition}

\begin{remark}[Notes on the definition]
  Given a game $A=(M_A,\lambda_A,\zeta_A,P_A)$, define $b_A=\neg\zeta_A(\emptyplay)$.  Then every play in $P_A$ must start with a $b_A$-move, so $A$ is either positive or negative.

  Note that $\zeta_A$ is now completely specified by $\lambda_A$ and $b_A$, so we could have specified our games mor efficiently by replacing $\zeta_A$ with $b_A$ in our definition, as done in \cite{martinsthesis}.  The slightly more unwieldy $\zeta_A$ will be useful when we come to extend our games over the ordinals, though, so we retain it.

  If $a\in M_A$ then we may recover $\lambda_A(a)$ from $\zeta_A$ so long as $a$ occurs in some play in $P_A$.  Since moves that can never be played do not affect the game at all, we do not really need $\lambda_A$ in our definition, but we keep it to make the connection to earlier work clearer.

  If $a\in M_A$ and $\lambda_A(a)=O$, we call $a$ an \emph{$O$-move}.  If $\lambda_A(a)=P$, we call $a$ a \emph{$P$-move}.  If $s\in P_A$ and $\zeta_A(s)=O$, we call $s$ an \emph{$O$-play} or \emph{$O$-position}.  If $\zeta_A(s)=P$, we call $s$ a \emph{$P$-play} or \emph{$P$-position}.

\end{remark}

We give the usual definition of a strategy as an appropriate subset of $P_A$, where we have identified the strategy with the set of all plays that can arise when player $P$ plays according to that strategy:

\begin{definition}
  Let $A=(M_A,\lambda_A,\zeta_A,P_A)$ be a game.  A \emph{strategy} for $A$ is a non-empty prefix-closed subset $\sigma\subset P_A$ such that:
  \begin{itemize}
    \item If $a\in P_A$ is an $O$-move and $s\in\sigma$ is a $P$-position such that $sa\in P_A$, then $sa\in\sigma$.
    \item If $s\in\sigma$ is an $O$-position and $a,b\in M_A$ are $P$-moves such that $sa,sb\in\sigma$, then $a=b$.
  \end{itemize}
\end{definition}

The definition above will be the most technically useful, but it is also convenient to give a strategy by a partial function that tells player $P$ which move to make in each $O$-position.  If we write $P_A^-=\zeta_A\inv(\{O\})$ for the set of all $O$-positions and $M_A^+=\lambda_A\inv(\{P\})$ for the set of all $P$-moves, then the strategy $\sigma$ above gives rise to a partial function $\hat\sigma\from P_A^-\pfun M_A^+$ given by:
\[
  \hat\sigma(s)=
  \begin{cases}
    a & \textrm{if $a\in M_A^+$ and $sa\in P_A$}\\
    \textrm{undefined} & \textrm{if $sb\not\in P_A$ for all $b\in M_A^+$}
  \end{cases}
  \]
The second condition on strategies tells us that this is well defined.  Going in the other direction, if we are given a partial function $f\from P_A^-\pfun M_A^+$  then we can define a strategy $\overline{f}$ by
\[
  \overline f = \{s\in M_A^*\suchthat \textrm{for all $ta\prefix s$ with $\zeta_A(t)=O$, $f(t)$ is defined and equal to $a$}\}
  \]

\subsection{Connectives}

Our definitions of connectives on games are as in \cite{blassgames}.

\begin{definition}
  Let $A=(M_A,\lambda_A,\zeta_A,P_A)$ be a game.  The negation of $A$, $\neggame{A}$, is the game formed by interchanging the roles of the two players.
  \begin{itemize}
    \item $M_{\bneggame A}=M_A$
    \item $\lambda_{\bneggame A} = \comp\neg{\lambda_A}$
    \item $\zeta_{\bneggame A} = \comp\neg{\zeta_A}$
    \item $P_{\bneggame A} = P_A$
  \end{itemize}
\end{definition}

It follows immediately from the definitions that $\neggame A$ is a well formed game.

We now define the tensor product $A\tensor B$, the par $A\parr B$ and the linear implication $A\implies B$ of two games.  All these games are obtained by playing $A$ and $B$ in parallel, so they all have the same set of moves:
\[
  M_{A\tensor B} = M_{A\parr B} = M_{A\implies B} = M_A \cprd M_B
  \]

Ownership of moves is decided via the obvious copairing functions:
\begin{IEEEeqnarray*}{c}
  \lambda_{A\tensor B} = \lambda_{A\parr B} = \lambda_A \cprd \lambda_B\\
  \lambda_{A\implies B} = (\neg\circ\lambda_A) \cprd \lambda_B
\end{IEEEeqnarray*}

We define the set $P_A\|P_B$ to be the set of all plays in $(M_A \cprd M_B)^*$ whose $M_A$-component is a play from $P_A$ and whose $M_B$-component is a play from $P_B$:
\[
  P_A\|P_B = \{s\in (M_A\cprd M_B)^*\suchthat s\vert_A\in P_A,\; s\vert_B\in P_B\}
  \]

We are now in a position to define $\zeta_{A\tensor B},\zeta_{A\parr B},\zeta_{A\implies B}$ as functions $P_A\|P_B\to\OP$:
\begin{IEEEeqnarray*}{rCl}
  \zeta_{A\tensor B}(s) & = & \zeta_A(s\vert_A) \wedge \zeta_B(s\vert_B) \\
  \zeta_{A\parr B}(s) & = & \zeta_A(s\vert_A) \vee \zeta_B(s\vert_B) \\
  \zeta_{A\implies B}(s) & = & \zeta_A(s\vert_A) \Rightarrow \zeta_B(s\vert_B)
\end{IEEEeqnarray*}

(Here, $\wedge$, $\vee$ and $\Rightarrow$ are as in Figure \ref{truthtables}.)

As things stand, $P_A\|P_B$ is not a valid set of plays for our $\lambda$ and $\zeta$ functions.  The first problem is that $P_A\|P_B$ contains plays which are not alternating with respect to $\zeta_{A\tensor B},\zeta_{A\parr B},\zeta_{A\implies B}$.  The second problem is that the $\zeta$ and $\lambda$ functions do not always agree with one another.  For example, suppose that $Q,R$ are two positive games.  So we have $\zeta_Q(\emptyplay) = \zeta_R(\emptyplay) = O$.  Then $\zeta_{Q\tensor R}(\emptyplay) = O\wedge O = O$.  But now suppose that player $P$ can make an opening move $q$ in $Q$.  Then we have
\[
  \zeta_{Q\tensor R}(q) = \zeta_Q(q) \wedge \zeta_R(\emptyplay) = P \wedge O = O
  \]
but $\lambda_{Q\tensor R}(q) = P$.  

Our solution is to throw away some plays from $P_A\| P_B$ so that what remains satisfies the alternating condition and the condition on the $\lambda$ and $\zeta$ functions.

\begin{definition}
  Let $M$ be a set, let $P\subset M^*$ be prefix closed and let $\cmap{\lambda}{M}{\OP},\cmap{\zeta}{P}{\OP}$ be functions.  If $s\in P$, we say that $s$ is \emph{alternating with respect to $\zeta$} if the set of prefixes of $s$ satisfies the alternating condition: if $t,ta\prefix s$, then $\zeta(t) = \neg\zeta(ta)$.

  We say that $s$ is \emph{well formed with respect to $\lambda,\zeta$} if $s = \emptyplay$ or if $s=s'a$ and $\zeta(s)=\lambda(a)$.

  Now let $A,B$ be games.  We define:
  \begin{IEEEeqnarray*}{rCl}
    P_{A\tensor B} & = & \left\{s\in P_A\|P_B \;\middle|\; \mbox{\pbox{\textwidth}{$s$ is alternating with respect to $\zeta_{A\tensor B}$ \\ $s$ is well formed with respect to $\zeta_{A\tensor B},\lambda_{A\tensor B}$}}\right\} \\
    P_{A\parr B} & = & \left\{s\in P_A\|P_B \;\middle|\; \mbox{\pbox{\textwidth}{$s$ is alternating with respect to $\zeta_{A\parr B}$ \\ $s$ is well formed with respect to $\zeta_{A\parr B},\lambda_{A\parr B}$}}\right\} \\
    P_{A\implies B} & = & \left\{s\in P_A\|P_B \;\middle|\; \mbox{\pbox{\textwidth}{$s$ is alternating with respect to $\zeta_{A\implies B}$ \\ $s$ is well formed with respect to $\zeta_{A\implies B},\lambda_{A\implies B}$}}\right\}
  \end{IEEEeqnarray*}
\end{definition}

\begin{definition}
  We define:
  \begin{IEEEeqnarray*}{cCc}
    A\tensor B & = & (M_{A\tensor B}, \lambda_{A\tensor B}, \zeta_{A\tensor B}, P_{A\tensor B}) \\
    A\parr B & = & (M_{A\parr B}, \lambda_{A\parr B}, \zeta_{A\parr B}, P_{A\parr B}) \\
    A\implies B & = & (M_{A\implies B}, \lambda_{A\implies B}, \zeta_{A\implies B}, P_{A\implies B})
  \end{IEEEeqnarray*}
\end{definition}

\begin{proposition}
  $A\tensor B$,$A\parr B$,$A\implies B$ are well formed games.  Moreover, $P_{A\tensor B}$ is the largest subset of $P_A\|P_B$ such that $(M_{A\tensor B}, \lambda_{A\tensor B}, \zeta_{A\tensor B}, P_{A\tensor B})$ is a well formed game and similarly for the connectives $\parr$ and $\implies$.  
  \begin{proof}
    We will prove the proposition for $A\tensor B$; the other two cases are entirely similar.  Alternatively, observe that $A\parr B=\neggame{(\neggame A \tensor \neggame B)}$ and $A\implies B = \neggame A \parr B$.

    For $A\tensor B$, it suffices to show that $P_{A\tensor B}$ is alternating with respect to $\zeta_{A\tensor B}$, since every $s\in P_{A\tensor B}$ is well-formed by definition.  Suppose $s,sa\in P_{A\tensor B}$; then $s,sa\prefix sa$; since $sa$ is alternating with respect to $\zeta_{A\tensor B}$, it follows that $\zeta_{A\tensor B}(s) = \neg\zeta_{A\tensor B}(sa)$.

    For the second part of the proposition, suppose that $V\subset P_A\|P_B$ is prefix-closed and satisfies the alternating condition with respect to $\zeta_{A\tensor B}$ and that every $s\in V$ is well-formed with respect to $\lambda_{A\tensor B},\zeta_{A\tensor B}$.  We need to show that $V\subset P_{A\tensor B}$, for which it will suffice to show that every $s\in V$ is alternating.  This is easy to see: since $V$ is prefix closed, the set of all prefixes of $s$ is a subset of $V$, and so it satisfies the alternating condition with respect to $\zeta_{A\tensor B}$.  
  \end{proof}
\end{proposition}

A design feature of the connectives $\tensor,\parr,\implies$ is that only player $O$ may switch games in $A\tensor B$, while only player $P$ may switch games in $A\parr B$ and $A\implies B$.  The $\implies$ case follows immediately from the $\parr$ case by noting that $A\implies B = \neggame A\parr B$, and the $\parr$ case then follows from the $\tensor$ case by observing that $A\parr B=\neggame{(\neggame A \tensor \neggame B)}$.  Thus, it will suffice to prove the following proposition for the tensor product:

\begin{proposition}
  \label{WhoSwitchesGames}
  Let $A,B$ be games.  Suppose $s\in P_{A\implies B}$, $a\in M_A$ and $b\in M_B$.  Then:

  i) If $sab\in P_{A\tensor B}$ then $\lambda_{A\tensor B}(b)=O$

  ii) If $sba\in P_{A\tensor B}$ then $\lambda_{\tensor B}(a)=O$.  

  \begin{proof}

    (i): $\begin{aligned}[t]
      \lambda_{A\tensor B}(b) & = \zeta_{A\tensor B}(sab) \\
       & = \zeta_{A\tensor B}(s\vert_A a) \wedge \zeta_{A\tensor B}(s\vert_B b) \\
       & = \lambda_{A\tensor B}(a) \wedge \lambda_{A\tensor B}(b)
    \end{aligned}$

    By alternation, either $\lambda_{A\tensor B}(a)=O$ or $\lambda_{A\tensor B}(b)=O$, so this last expression must be equal to $O$.
    
    (ii): \noindent$\begin{aligned}[t]
      \lambda_{A\tensor B}(a) & = \zeta_{A\tensor B}(sba) \\
       & = \zeta_{A\tensor B}(s\vert_A a) \wedge \zeta_{A\tensor B}(s\vert_B b) \\
       & = \lambda_{A\tensor B}(a) \wedge \lambda_{A\tensor B}(b) = O\textrm{ (by the same argument)} && \quad\;\qedhere
    \end{aligned}$
  \end{proof}
\end{proposition}

\subsection{A category of games and partial strategies}

Following \cite{joyalgames} and \cite{abramskyjagadeesangames}, we define a category $\G$ whose objects are games where the morphisms from a game $A$ to a game $B$ are strategies for $A\implies B$.  For the sake of simplicity, and to avoid varioius technical issues, we shall require that the games in our category be \emph{negative} - namely, they should start with an opponent move.  In our language, we call a game $A$ \emph{negative} if $\zeta_A(\emptyplay)=P$ (and we call it positive if $\zeta_A(\emptyplay)=O$).  The equations
\begin{gather*}
  P \wedge P = P \\
  P \Rightarrow P = P
\end{gather*}
tell us that the class of negative games is closed under $\tensor$ and $\implies$ (since $P\vee P=P$, it is also closed under $\parr$, but the par of two negative games has no legal moves in our presentation, so it does not give a good model of the $\parr$ connective in linear logic and we will not consider it).

In order to get a category, we need a way to compose a strategy for $A\implies B$ with a strategy for $B\implies C$.  Our treatment of composition is heavily influenced by work done by Hyland and Schalk (see \cite{hyland1997games} and \cite{hylandschalkgames}).  

We shall use the usual definition of composition.  If $\sigma$ is a strategy for $A\implies B$ and $\tau$ is a strategy for $B\implies C$, we define a set
\[
  \sigma\|\tau = \{\s\in (M_A \cprd M_B \cprd M_C)^*\suchthat \s\vert_{A,B}\in\sigma,\;\s\vert_{B,C}\in\tau\}
  \]
Then we define the composite strategy on $A\implies C$ to be
\[
  \comp\tau\sigma = \{\s\vert_{A,C}\suchthat\s\in\sigma\|\tau\}
  \]
The process of showing that this is indeed a strategy for $A\implies C$, and that composition is associative, will be quite involved.  We shall start with a surprising lemma about strategies on the implication $A\implies B$.  

\begin{lemma}
  \label{PlayInStrategyDeterminedByComponents}
  et $A,B$ be games and let $\sigma$ be a strategy for $A\implies B$.  Suppose that $s,t\in\sigma$ are such that $s\vert_A=t\vert_A$ and $s\vert_B=t\vert_B$.  Then $s=t$.
  \begin{proof}
    Suppose that $s\ne t$.  Let $r\prefix s,t$ be the longest common subsequence of $s$ and $t$ - so we have $rx\prefix s, ry\prefix t$ for some moves $x,y\in M_{A\implies B}$ with $x\ne y$.  Since $rx,ry$ are both substrings of the strategy $\sigma$, $r$ must be a $P$-position and therefore the moves $x,y$ must take place in the same game.  Without loss of generality, suppose that $x,y\in M_A$.  Then we have $r\vert_Ax\prefix s\vert_A,r\vert_Ay\prefix t\vert_A$ and so $s\vert_A\ne t\vert_A$.
  \end{proof}
\end{lemma}

Thus we see that the plays according to some given strategy $\sigma$ for $A\implies B$ are characterized by their $A$- and $B$-components.  More is true, however: given negative games $A$ and $B$, the strategies on $A\implies B$ are characterized by the $A$- and $B$-components of their constituent plays:
\begin{lemma}
  \label{HylandSchalkFaithful}
  Let $A,B$ be negative games and let $\sigma,\sigma'$ be strategies for $A\implies B$ such that
  \[
    \{(s\vert_A,s\vert_B)\suchthat \textrm{$s\in\sigma$ is a $P$-position}\}=\{(s'\vert_A,s'\vert_B)\suchthat\textrm{$s'\in\sigma'$ is a $P$-position}\}
    \]
  Then $\sigma=\sigma'$.
  \begin{proof}
    Suppose for a contradiction that $\sigma\ne\sigma'$, so without loss of generality there is some $P$-position $s\in\sigma\setminus\sigma'$ (using the definition of a strategy).  By hypothesis, there is some $s'\in\sigma'$ such that $s'\vert_A=s\vert_A$ and $s'\vert_B=s\vert_B$.  Clearly, $s\ne s'$, since $s\ne\sigma'$, so let $r\prefix s$ be the largest common prefix of $s$ and $s'$.  We therefore have $rx\prefix s,ry\prefix s'$, where $x,y\in M_{A\implies B}$ and $x\ne y$.  

    Since $s\not\in\sigma'$, $rx\not\in\sigma'$.  Since $ry\in\sigma'$, $x$ and $y$ must be $P$-moves by the definition of a strategy.  Suppose without loss of generality that $x\in M_A$.  Then $y$ cannot also be an $A$-move, or we would have $r\vert_Ax\prefix s\vert_A$ and $r\vert_Ay\prefix s'\vert_A=s\vert_A$, which would contradict the fact that $x\ne y$.  So $y\in M_B$.  Now, by hypothesis, there exists some $P$-position $t\in\sigma$ such that $t\vert_A=ry\vert_A=r\vert_A$ and $t\vert_B=ry\vert_B=r\vert_By$.

    Now clearly $rx\ne t$ (since their restrictions to $A$ and $B$ are different).  Let $q\prefix rx, t$ be their largest common prefix; then we have $qu\prefix rx, qv\prefix t$ for some $u\ne v$.  But now since $qu, qv\in\sigma$, $u$ and $v$ must both be $O$-moves, so they must be contained in the same component.  But this is impossible: if they are both $A$-moves then we have $q\vert_Au\prefix r\vert_Ax$ and $q\vert_Av\prefix r\vert_A$, so $u=v$ and if they are both $B$-moves then we have $q\vert_Bu\prefix r\vert_B$ and $q\vert_Bv\prefix r\vert_By$, so $u=v$ again.  This is a contradiction.
  \end{proof}
\end{lemma}

Given games $A,B$ and a strategy $\sigma$ for $A\implies B$, the set
\[
  \{(s\vert_A,s\vert_B)\suchthat \textrm{$s\in\sigma$ is a $P$-position}\}\subset P_A\times P_B
  \]
is a \emph{relation} between $P_A$ and $P_B$.  Writing $\grel\sigma\subset P_A\times P_B$, we will show that if $A,B,C$ are games and $A\xrightarrow{\sigma}\map{\tau}{B}{C}$ are morphisms, then:
\[
  \grel{\comp\tau\sigma}=\comp{\grel\tau}{\grel\sigma}
  \]
under the usual composition of relations:
\[
  \comp{\grel\tau}{\grel\sigma} = \{(s,t)\in P_A\times P_C\suchthat \exists u\in P_B\esuchthat (s,u)\in\grel\sigma,\;(u,t)\in\grel\tau\}
  \]
Thus we will get a functor $\cmap{\F}{\G}{\Rel}$, where $\G$ is our category of negative games and $\Rel$ is the category of sets and relations.  Proposition \ref{HylandSchalkFaithful} then tells us that this functor is faithful.  

It is clear from the definition of $\comp\tau\sigma$ that $\grel{\comp\tau\sigma}\subset\comp{\grel\tau}{\grel\sigma}$; indeed if $\s\in\sigma\|\tau$, then $(\s\vert_A,\s\vert_B)\in\grel\sigma$ and $(\s\vert_B,\s\vert_C)\in\grel\tau$, so $(\s\vert_A,\s\vert_C)\in\comp{\grel\tau}{\grel\sigma}$.  To show the reverse inclusion, we need to show the following: if $s\in\sigma$ and $t\in\tau$ are such that $s\vert_B=t\vert_B$, then there is some $\s\in (M_A\cprd M_B\cprd M_C)^*$ such that $\s\vert_{A,B}=s$ and $\s\vert_{B,C}=t$.  

This can be shown in an easy way.  Roughly speaking, we write out the sequence $s$, and then insert immediately after each $B$-move the $C$-moves from $t$ that occur after that $B$-move.  This technique works for arbitrary sequences and is sufficient to prove that $\grel{\comp\tau\sigma}=\comp{\grel\tau}{\grel\sigma}$.  However, it turns out that if $s$ and $t$ are alternating plays in $A\implies B$ and $B\implies C$ then this interleaving is unique - and even more is true:

\begin{lemma}
  \label{LiftingLemma}
  Let $A,B,C$ be negative games, let $\sigma$ be a strategy for $A\implies B$ and let $\tau$ be a strategy for $B\implies C$.  Suppose that $s\in\sigma$ and $t\in\tau$ are such that $s\vert_B=t\vert_B$.  Suppose moreover that $s$ and $t$ do not both end with $B$-moves.  Then there is a unique sequence $\s\in(M_A\cprd M_B\cprd M_C)^*$ such that:
  \begin{itemize}
    \item $\s\in\sigma\|\tau$ (so $\s\vert_{A,B}\in\sigma$ and $\s\vert_{B,C}\in\tau$)
    \item $\s$ does not end with a $B$-move
    \item $\s\vert_A=s\vert_A$
    \item $\s\vert_C=t\vert_C$
  \end{itemize}
  Moreover, for this $\s$ we have $\s\vert_{A,B}=s$ and $\s\vert_{B,C}=t$.
\end{lemma}

Note that the statement of Lemma \ref{LiftingLemma} is stronger than saying that there is a unique $\s$ such that $\s\vert_{A,B}=s$ and $\s\vert_{B,C}=t$.  Instead, we are saying that even if we make no restriction on the $B$-component of $\s$ (other than that $\s\in\sigma\|\tau$) then $\s$ is still uniquely determined, and it still restricts to $s$ and $t$ if we restrict to $A\implies B$ or $B\implies C$.

\begin{proof}[Proof of Lemma \ref{LiftingLemma}]
  Let $n=\length(s)+\length(t\vert_C)$; i.e., the total number of unique terms in $s$ and $t$ where we have identified the sequences $s\vert_B$ and $t\vert_B$ (this will be the length of the interleaved sequence $\s$).  We first prove that for each $k$ with $0\le k\le n$, there exists a unique sequence $\s^k\in\sigma\|\tau$ of length $k$ such that $\s^k\vert_A\prefix s\vert_A$ and $\s^k\vert_C\prefix t\vert_A$.  Moreover for this $\s^k$ we have $\s^k\vert_{A,B}\prefix s$ and $\s^k\vert_{B,C}\prefix t$, and if $j\le k$, then $\s^j\prefix\s^k$.

  We prove this by induction on $k$.  There is a unique sequence of length $0$, namely the empty sequence, and its $A$- and $C$-components are both empty, so are prefixes of $s$ and $t$.  

  Suppose now that $k=1$.  We make the observation that if $a$ is an $O$-move in $A$ and if $b$ is an $O$-move in $B$ then $a$ is a $P$-move in $A\implies B$ and $b$ is a $P$-move in $B\implies C$.  Since the starting move in $A\implies B$ or $B\implies C$ must be an $O$-move in both its component game and in the game as a whole, we see that any sequence in $\sigma\|\tau$ must begin with a move in $C$.  So we have $\s^1=c$, for some $C$-move $c$, and the condition that $\s^1\vert_C\prefix t$ means that $c$ must be the first move in $t$, which is always a move from $C$.  We have $\s^1\vert_A=\emptyplay\prefix s\vert_A$ and $\s^1\vert_B=\emptyplay\prefix s\vert_B$, as desired.

  Now suppose that we have constructed the sequence $\s^k$ for some $k$ such that $1\le k<n$.  We seek a sequence $\s^{k+1}\in \sigma\|\tau$ of length $k+1$ such that $\s^{k+1}\vert_A\prefix s$ and $\s^{k+1}\vert_C\prefix t$.  If we write $\s'$ for the sequence obtained by removing the last move in $\s^{k+1}$, it is clear that $\s'\vert_A\prefix s$ and $\s'\vert_C\prefix t$, so $\s'=\s^k$ by uniqueness.  Therefore, $\s^{k+1}$ is of the form $\s^kx$, for some move $x\in M_A\cprd M_B\cprd M_C$.  

  Write $\s^k=\s''y$, where $y$ is the last move in $\s^k$.  Our move $x$ will depend on which game $y$ is played on as follows:
  
  \begin{itemize}
    \item Suppose $y$ is a move in $A$ or an $O$-move in $B$.  By induction, $\s''y\vert_{A,B}$ is a prefix of $s$ ending in $y$.  We claim that it is a proper prefix.  

      Indeed, let $b$ be the last $B$-move occurring in $\s''y$ (since play in $A\implies B$ starts with a $B$-move, there must be such a move).  If $b=y$, then $b$ is an $O$-move in $A\implies B$.  Otherwise, $y$ must be a move in $A$, and then $b$ is again an $O$-move in $A\implies B$, since player $P$ switches games.  Therefore, $b$ is a $P$-move in $B\implies C$.  Now $\s''y\vert_B$ is a prefix of $t$ ending in $b$.  Since $b$ is a $P$-move in $B\implies C$, it may only be followed in $B\implies C$ by another move from $B$, so there are two possibilities: either $\s''y\vert_{B,C}=t$ or there is some other $B$-move $b'$ that occurs later than $b$ in the sequence $t$.  In the first case, $\s''y\vert_{A,B}$ must be a proper prefix of $s$ by length considerations:
      \begin{align*}
        \length(\s''y\vert_{A,B}) & = \length(\s''y) - \length(\s''y\vert_C) \\
          & = \length(\s''y) - \length(t\vert_C) \\
          & = k - \length(t\vert_C) < n - \length(t\vert_C) = \length(s)
      \end{align*}
      while in the second case $\s''y\vert_B$ must be a proper prefix of $t\vert_B=s\vert_B$, and so $\s''y\vert_{A,B}$ must be a proper prefix of $s$.

      Therefore, we have $\s''y\vert_{A,B}x\prefix s$ for some move $x\in M_A\cprd M_B$.  If $x$ is an $A$-move then $\s''yx\vert_{B,C}=\s''y\vert_{B,C}\prefix t$, as desired.  If $x$ is a move in $B$, then it must be a $P$-move in $A\implies B$ (since $y$ is either a move in $A$ or an $O$-move in $B$), so it is an $O$-move in $B\implies C$.  Since $s\vert_B=t\vert_B$, we must have $\s''yx\vert_B\prefix t\vert_B$, and so $y$ must be a $P$-move in $B\implies C$, and should therefore be followed by another move in $B$, which must be $x$.  Therefore, $\s''yx\vert_{B,C}\prefix t$.

      For uniqueness, suppose that $\s''yz\in\sigma\|\tau$ is such that $\s''yz\vert_A\prefix s\vert_A$ and $\s''yz\vert_C\prefix t\vert_C$.  As before, let $b$ be the last $B$-move occurring in $\s''y$; we have already shown that $b$ must be a $P$-move in $B\implies C$, and it follows that it must be the last move occurring in $\s''y\vert_{B,C}$ (since it can only be followed by another $B$-move).  Suppose that $z\in M_B\cprd M_C$.  Then $\s''yz\vert_{B,C}=\s''y\vert_{B,C}z$; since the last move in $\s''y\vert_{B,C}$ is $b$, this means that $z$ must be an $O$-move in $B$.  So $z$ is either an $O$-move in $B$ or a move in $A$.

      If $y$ was an $O$-move in $A$, then $y$ is a $P$-move in $A\implies B$, so it must be followed in $A\implies B$ by another move from $A$.  Then the condition that $\s''yz\vert_A\prefix s\vert_A$ tells us that $z=x$.  If instead $y$ was a $P$-move in $A$ or a move in $B$ then $y$ is an $O$-move in $A\implies B$; now, since we have $\s''y\vert_{A,B}z,\s''y\vert_{A,B}x\in\sigma$, it must be the case that $x=z$ by the definition of a strategy.

    \item If instead $y$ is a move in $C$ or a $P$-move in $B$, we use a symmetrical argument, choosing $x$ to be the next move along in $t$.  We need to take a little extra care here when we talk about the last $B$-move in $\s''y$; there may in fact be no $B$-moves in $\s''y$.  Since any play in $A\implies B$ must begin with a $B$-move, this only happens when $\s''y$ is entirely made up of $C$-moves.  In this case, it is not difficult to show that $\s''y\vert_{B,C}$ is a proper prefix of $t$ by length considerations. 
  \end{itemize}

  Having constructed the sequences $\s^k$, we now claim that $\s=\s^n$ is the sequence we desire.  We have $\s\vert_{A,B}\prefix s$ and $\s\vert_{B,C}\prefix t$, and a length argument tells us that these prefix relations are actually equalities:
  \[
    \length(s)=n-\length(t\vert_C)\le n-\length(\s\vert_C)=\length(\s\vert_{A,B})\le\length(s)
    \]
  so we must have equality everywhere, and similarly for $t$ and $\s\vert_{B,C}$.  Since at least one of $s,t$ does not end with a $B$-move, our interleaved sequence $\s$ cannot end with a $B$-move.  

  Now suppose that $\t$ were some other sequence in $\sigma\|\tau$ such that $\t\vert_A=s\vert_A$ and $\t\vert_C=t\vert_C$.  If $\length(\t)\le n$, then $\t=\s^k$ for some $k$ by uniqueness above.  So $\t\prefix\s$.  We also know that $\t$ includes the last move from $\s$, since this move must be an $A$-move or a $C$-move.  Therefore, $\t=\s$.  

  If instead $\length(\t)\ge n$ then let $\t'$ be the length-$n$ prefix of $\t$.  By uniqueness above, we must have $\t'=\s$.  Since $\t\vert_A=\s\vert_A$ and $\t\vert_C=\s\vert_C$, this means that all subsequent moves in $\t$ must be $B$-moves.  Since $\t$ does not end with a $B$-move, we must have $\t=\s$.
\end{proof}

\begin{lemma}
  \label{HylandSchalkIsFunctor}
  Let $A,B,C$ be games, let $\sigma$ be a strategy for $A\implies B$ and let $\tau$ be a strategy for $B\implies C$.  Suppose there exist $s\in\sigma,t\in\tau$ such that $s\vert_B=t\vert_B$.  Then there is a unique \emph{interleaving} of $s$ and $t$; i.e., there is a unique sequence $\s\in (M_A\cprd M_B\cprd M_C)^*$ such that $\s\vert_{A,B}=s$ and $\s\vert_{B,C}=t$.  
\end{lemma}
\begin{remark}
  The existence part of this lemma holds for arbitrary sequences in an easy way, but uniqueness does not: for example, the sequences $ab$ and $cb$ have two distinct interleavings ($acb$ and $cab$).  Existence is all we need to show that $\grel{\comp\tau\sigma}=\comp{\grel\tau}{\grel\sigma}$, but uniqueness will come in useful later on, so we prove it now.
\end{remark}
\begin{proof}[Proof of Lemma \ref{HylandSchalkIsFunctor}]
  Let $A,B,C$ be games, let $\sigma$ be a strategy for $A\implies B$ and let $\tau$ be a strategy for $B\implies C$.  Let $s\in\sigma,t\in\tau$ be such that $s\vert_B=t\vert_B$.  

  We build a sequence $\s\in (M_A\cprd M_B\cprd M_C)^*$ inductively.  The first move in $\s$ is the first move in $t$, which is our only choice since the first move in $s$ is a $P$-move in $B\implies C$.  Now suppose that we have built up a non-empty sequence $\s'$ such that $\s'\vert_{A,B}\prefix s$ and $\s'\vert_{B,C}\prefix t$.  We extend $\s'$ by one move $x$ such that $\s'x\vert_{A,B}\prefix s$ and $\s'x\vert_{B,C}\prefix t$.  Moreover, we show that there is a unique such $x$.  

  Since $\s'$ is non-empty, we may write it as $\s'=\s''y$.  The move $x$ depends on $y$ as follows:
  \begin{itemize}
    \item If $y$ is an $O$-move in $A$, then $y$ is a $P$-move in $A\implies B$.  By induction, $\s''y\vert_{A,B}$ is a prefix of $s$ ending in $y$.  If in fact $\s''y\vert_A=s$, then terminate.  Otherwise, $x$ is the next move in $s$ after $\s''y\vert_{A,B}$.  
      
      Since $y$ is a $P$-move in $A\implies B$, $x$ must be a move in $A$ as well, by Proposition \ref{WhoSwitchesGames}.  Therefore, $\s'x\vert_{B,C}=\s'\vert_{B,C}\prefix t$, while $\s'x\vert_{A,B}\prefix s$ by the choice of $x$.  

      For uniqueness, suppose that $\s'z\vert_{A,B}\prefix s$ and $\s'z\vert_{B,C}\prefix t$.  If $z$ is a move in $B$ or $C$, then we have
      \[
        \s''z\vert_{B,C}=\s''yz\vert_{B,C}\prefix t
        \]
      since $y$ is a move in $A$, but this contradicts the uniqueness of $y$.  Therefore, $z$ must be a move in $A$, and the condition that $\s'z\vert_{A,B}\prefix s$ forces $z$ to be equal to $x$.  

    \item If $y$ is a $P$-move in $A$ or an $O$-move in $B$, then $y$ is an $O$-move in $A\implies B$.  By induction, $\s''y\vert_{A,B}$ is a prefix of $s$ ending in $y$.  If $\s''y\vert_A=s$, then terminate.  Otherwise, $x$ is the next move in $s$ after $\s''y\vert_{A,B}$.  

      Since $s\in\sigma$, which is a strategy, and $y$ is an $O$-move in $A\implies B$, $x$ must be uniquely determined.  Moreover, we have $\s'x\vert_{A,B}\prefix s$ by the choice of $x$.  
  \end{itemize}

\end{proof}
\bibliographystyle{alpha}
\bibliography{ordinal_games}

\end{document}
